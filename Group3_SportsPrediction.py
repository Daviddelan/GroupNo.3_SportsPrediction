# -*- coding: utf-8 -*-
"""sports_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uwGn7NaINJSp-chq6pek5cN4AI7makKH
"""

!pip install --upgrade scikit-learn

import pandas as pd
import numpy as np
import os
import sklearn
import numpy as np
import matplotlib.pyplot as plt
from sklearn import tree, metrics
from sklearn.model_selection import train_test_split
import pickle

from google.colab import drive
drive.mount('/content/drive')

"""Loading the Data"""

players_21 = pd.read_csv('/content/drive/My Drive/players_21.csv')

players_21

players_21.shape

"""Code to show all the various elements and their datatypes

"""

for col in players_21.columns:
    print(f"{col}: {players_21.dtypes[col]}")

"""Code to single out all variables that are of type "object"
"""

for col in players_21.columns:
    if players_21.dtypes[col] == 'object':
        print(col)

"""Dropped ls - gk ( In chronological order according to the first code) since they do not really contribute to the overall rating. They are the various ratings of players if they move to different positions.

Also dropped all the urls since they do not play a role in determining the overall rating of the player

Also dropped short_name, long_name since the players names do not usually play a role in determining the rating of the player.

We dropped player_positions since it had multiple positions for just one player which will cause the one hot encoding to be unnecessarily long. Therefore we used club_position instead since it contains just one position per each player.

Also, we dropped dob since it is the full data of birth of the player. However we have the age of the player which will help since the age of the player in most cases determines the effectiveness of the player and hence the value of the player.
"""

selected_columns = ["club_name", "league_name", "club_position", "work_rate", "body_type", "preferred_foot"]
categorical_values = players_21[selected_columns].copy()
categorical_values

new_players21 = players_21.copy()

categorical_columns = [
    "player_url",
    "short_name",
    "long_name",
    "player_positions",
    "dob",
    "club_name",
    "league_name",
    "club_position",
    "club_loaned_from",
    "club_joined",
    "nationality_name",
    "nation_position",
    "preferred_foot",
    "work_rate",
    "body_type",
    "real_face",
    "player_tags",
    "player_traits",
    "ls",
    "st",
    "rs",
    "lw",
    "lf",
    "cf",
    "rf",
    "rw",
    "lam",
    "cam",
    "ram",
    "lm",
    "lcm",
    "cm",
    "rcm",
    "rm",
    "lwb",
    "ldm",
    "cdm",
    "rdm",
    "rwb",
    "lb",
    "lcb",
    "cb",
    "rcb",
    "rb",
    "gk",
    "player_face_url",
    "club_logo_url",
    "club_flag_url",
    "nation_logo_url",
    "nation_flag_url"
]

new_players21.drop(columns =categorical_columns, inplace = True )

new_players21

from sklearn.impute import SimpleImputer

imp = SimpleImputer(strategy = "mean")
imputed_data = imp.fit_transform(new_players21)
new_players21 = pd.DataFrame(imputed_data, columns=new_players21.columns)



encoded_categoric = pd.get_dummies(categorical_values)
imp = SimpleImputer(strategy = "most_frequent")
imputed_data = imp.fit_transform(encoded_categoric)
encoded_categorical = pd.DataFrame(imputed_data, columns=encoded_categoric.columns)
encoded_categorical.head(5)

encoded_players21 = pd.concat([new_players21, encoded_categorical], axis = 1)

encoded_players21.info()

from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

y = encoded_players21["overall"]
X = encoded_players21.drop("overall", axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.impute import SimpleImputer

# Train a Random Forest model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Feature Importance
encoder = OneHotEncoder()
encoded_data = encoder.fit_transform(X_train)
importances = model.feature_importances_
feature_names = encoder.get_feature_names_out(X_train.columns)
# Sort them in descending order
indices = np.argsort(importances)[::-1]

"Let's display the ranking of feature importance."
print("Feature ranking:")

for i in range(X.shape[1]):
    print(f"{i + 1} {X.columns[indices[i]]} ({importances[indices[i]]})")

# Get the top 5 most important features
top_n = 5
top_features = [X.columns[indices[i]] for i in range(top_n)]

# Create a DataFrame with the top features from 'encoded_players21'
final_players = encoded_players21[top_features]


final_players

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

encoded_players21

X = sc.fit_transform(final_players)
X = pd.DataFrame(X,columns = final_players.columns)
y = encoded_players21["overall"]
X

Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,y,test_size=0.2,random_state=42)

# from sklearn.ensemble import RandomForestClassifier

# rf=RandomForestClassifier(n_estimators=100,n_jobs=-1)

# rf.fit(Xtrain, Ytrain)

# from sklearn.metrics import accuracy_score

# accuracy_score(y_pred,Ytest)

# from sklearn.ensemble import VotingClassifier
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.svm import SVC

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# rf = RandomForestRegressor(n_estimators=150, random_state=42)
# rf.fit(Xtrain, Ytrain)
# y_pred = rf.predict(Xtest)
# y_pred

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV

from sklearn.model_selection import KFold

# Define the number of splits for KFold cross-validation
splits = 3  # You can adjust the number of splits as needed

# Create a KFold object
cv = KFold(n_splits=splits)

# Rest of your code (data preprocessing, parameter tuning, model training, etc.)

PARAMETERS = {
    "n_estimators": [50, 80, 110],
    "max_depth": [2, 5, 6, 12],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4],
    "max_features": ["auto", "sqrt", "log2"],
}

rf = RandomForestRegressor(random_state=42)

model_gs = GridSearchCV(rf, param_grid=PARAMETERS, cv=cv, scoring="neg_mean_squared_error")

model_gs.fit(Xtrain, Ytrain)

y_pred = model_gs.predict((Xtest))

y_pred

one_player = model_gs.predict(sc.transform([[103500000, 138400000, 33, 94.0, 90.0]]))
one_player

pickle.dump(model_gs, open('model1.pkl','wb'))

mse = mean_squared_error(Ytest, y_pred)
rmse = mean_squared_error(Ytest, y_pred, squared=False)  # squared=False to get RMSE
r2 = r2_score(Ytest, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"Root Mean Squared Error: {rmse}")
print(f"R-squared (R²): {r2}")

from sklearn.metrics import mean_absolute_error

# Calculate MAE
mae = mean_absolute_error(Ytest, y_pred)

print(f"Mean Absolute Error: {mae}")



pip install xgboost

from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV


PARAMETERS = {
    "n_estimators": [100, 200, 300],
    "max_depth": [2, 5, 6, 12],
    "min_child_weight": [1, 5, 15],
    "learning_rate": [0.3, 0.1, 0.03],
    "subsample": [0.5, 0.75, 1],
    "colsample_bytree": [0.5, 0.75, 1]
}

xgb = XGBRegressor(n_jobs=-1, random_state=42)

xgbmodel_gs = GridSearchCV(xgb, param_grid=PARAMETERS, cv=cv, scoring="neg_mean_squared_error")

xgbmodel_gs.fit(Xtrain, Ytrain)

y_pred = xgbmodel_gs.predict(Xtest)
print("Best Parameters:", model_gs.best_params_)

mse = mean_squared_error(Ytest, y_pred)
rmse = mean_squared_error(Ytest, y_pred, squared=False)  # squared=False to get RMSE
r2 = r2_score(Ytest, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"Root Mean Squared Error: {rmse}")
print(f"R-squared (R²): {r2}")

from sklearn.metrics import mean_absolute_error

# Calculate MAE
mae = mean_absolute_error(Ytest, y_pred)

print(f"Mean Absolute Error: {mae}")

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV

PARAMETERS = {
    "n_estimators": [100, 200, 300],
    "max_depth": [2, 5, 6, 12],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4],
    "max_features": ["auto", "sqrt", "log2"]
}

gbr = GradientBoostingRegressor(random_state=42)

gbmodel_gs = GridSearchCV(gbr, param_grid=PARAMETERS, cv=cv, scoring="neg_mean_squared_error")


gbmodel_gs.fit(Xtrain, Ytrain)

y_pred = gbmodel_gs.predict(Xtest)

mse = mean_squared_error(Ytest, y_pred)
rmse = mean_squared_error(Ytest, y_pred, squared=False)  # squared=False to get RMSE
r2 = r2_score(Ytest, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"Root Mean Squared Error: {rmse}")
print(f"R-squared (R²): {r2}")

from sklearn.metrics import mean_absolute_error

# Calculate MAE
mae = mean_absolute_error(Ytest, y_pred)

print(f"Mean Absolute Error: {mae}")

# refined_players21 = refined_players21.drop(columns = columns_to_drop1)

"""#The testing part of model"""

players_22 = pd.read_csv('/content/drive/My Drive/players_22.csv')

players_22

players_22.shape

players_22.info()

players_22.describe()

players_22.head()

players_22.corr()

corr_overall = players_22.corr()['overall']

selected_columns2 = ["club_name", "league_name", "club_position", "work_rate", "body_type", "preferred_foot"]
categorical_values2 = players_22[selected_columns2].copy()
categorical_values2

# refined_players21 = players_21.drop(columns = columns_to_drop)
# refined_players22 = corr_overall[corr_overall.abs()>0.35]

new_players22 = players_22.copy()

categorical_columns2 = [
    "player_url",
    "short_name",
    "long_name",
    "player_positions",
    "dob",
    "club_name",
    "league_name",
    "club_position",
    "club_loaned_from",
    "club_joined",
    "nationality_name",
    "nation_position",
    "preferred_foot",
    "work_rate",
    "body_type",
    "real_face",
    "player_tags",
    "player_traits",
    "ls",
    "st",
    "rs",
    "lw",
    "lf",
    "cf",
    "rf",
    "rw",
    "lam",
    "cam",
    "ram",
    "lm",
    "lcm",
    "cm",
    "rcm",
    "rm",
    "lwb",
    "ldm",
    "cdm",
    "rdm",
    "rwb",
    "lb",
    "lcb",
    "cb",
    "rcb",
    "rb",
    "gk",
    "player_face_url",
    "club_logo_url",
    "club_flag_url",
    "nation_logo_url",
    "nation_flag_url"
]

new_players22.drop(columns =categorical_columns2, inplace = True )

new_players22

imp = SimpleImputer(strategy = "mean")
imputed_data = imp.fit_transform(new_players22)
new_players22 = pd.DataFrame(imputed_data, columns=new_players22.columns)

from sklearn.impute import SimpleImputer

encoded_categoric2 = pd.get_dummies(categorical_values2)
imp = SimpleImputer(strategy = "most_frequent")
imputed_data = imp.fit_transform(encoded_categoric2)
encoded_categorical2 = pd.DataFrame(imputed_data, columns=encoded_categoric2.columns)
encoded_categorical2.head(5)

encoded_players22 = pd.concat([new_players22, encoded_categorical2], axis = 1)

top_features2 = ['value_eur','release_clause_eur','age','potential','movement_reactions']

final_players2 = encoded_players22[top_features2]


final_players2

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()


X = sc.fit_transform(final_players2)
X = pd.DataFrame(X,columns = final_players2.columns)
y = encoded_players22["overall"]


Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,y,test_size=0.2,random_state=42)
X
Ytest

predictions_22 = model_gs.predict(Xtest)

mse = mean_squared_error(Ytest, predictions_22)
rmse = mean_squared_error(Ytest, predictions_22, squared=False)
r2 = r2_score(Ytest, predictions_22)
mae = mean_absolute_error(Ytest,predictions_22)

print(f"Mean Squared Error: {mse}")
print(f"Root Mean Squared Error: {rmse}")
print(f"R-squared (R²): {r2}")
print(f"Mean Absolute Error: {mae}")

predictions_22

pickle.dump(model_gs, open('model1.pkl','wb'))

import pickle

# Specify an absolute path where you want to save the file
file_path = '/content/drive/My Drive/model.pkl'

with open(file_path, 'wb') as file:
    pickle.dump(model_gs, file)

!pip install flask-ngrok

import numpy as np
from flask import Flask, request, jsonify, render_template
from flask_ngrok import run_with_ngrok

app = Flask(__name__)
run_with_ngrok(app)

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json

    player_value = data.get('value')
    player_age = data.get('age')
    release_clause = data.get('releaseClause')
    movement_reaction = data.get('movementReaction')
    player_potential = data.get('potential')

    prediction = model.predict([[player_value,release_clause,player_age,player_potential,movement_reaction]])

    response = {'player_rating': prediction}
    return jsonify(response)

app.run()
    #app.run(host="0.0.0.0")            # deploy